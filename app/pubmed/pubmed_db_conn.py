"""
Allows dumping data into SQLite to speed up iteration
over the ~33 million records.
"""
from typing import Optional

import atomics
import neo4j

from app.pubmed.filtering import PubMedFilterCache
from app.pubmed.model import DBArticle, DBMetadata, DBMeSHHeading, DBAuthor, DBArticleAuthor, DBAffiliation
from app.config import NEO4J_URI, NEO4J_REQUIRES_AUTH


class IdCounter:
    """
    Increments IDs to use for database nodes. This is a bit dodgy,
    but we only ever add to the database from one process, so it
    should be fine. This is required as Neo4J does not have its
    own auto-incrementing IDs.
    """
    id: atomics.atomic = atomics.atomic(width=4, atype=atomics.INT)

    def __init__(self, initial_id: int = None):
        if initial_id is not None:
            self.id.store(initial_id)

    def next(self) -> int:
        return self.id.fetch_inc()


class PubMedCacheConn:
    """
    Can be used to connect to the pubmed cache Neo4J database.
    """
    def __init__(self, database: Optional[str] = None):
        self.database: Optional[str] = database
        self.driver: Optional[neo4j.Driver] = None
        self.filter_query_cache = PubMedFilterCache()

        # We store metadata about the database within a Metadata node.
        self.metadata: Optional[DBMetadata] = None

        # We maintain our own counters for the IDs of authors. This is required as we
        # cannot trust the IDs generated by Neo4J as they can change, nor the IDs from
        # PubMed as they often don't exist.
        self.author_id_counter: Optional[IdCounter] = None

        # We cache the MeSH headings, as they should almost never change.
        self._mesh_headings: Optional[list[DBMeSHHeading]] = None

    def clear_cached_mesh_headings(self):
        """
        Clears the cached MeSH headings.
        """
        self._mesh_headings = None

    def __enter__(self):
        if self.driver is not None:
            raise ValueError("Already created connection!")

        max_life = 1000 * 60 * 60 * 24
        if NEO4J_REQUIRES_AUTH:
            from app.config import NEO4J_USER, NEO4J_PASSWORD
            self.driver = neo4j.GraphDatabase.driver(
                NEO4J_URI,
                auth=(NEO4J_USER, NEO4J_PASSWORD),
                max_connection_lifetime=max_life
            )
        else:
            self.driver = neo4j.GraphDatabase.driver(NEO4J_URI, max_connection_lifetime=max_life)

        # Create a connection to the database to create its constraints and grab metadata.
        with self.new_session() as session:
            self.create_constraints(session)
            self._fetch_metadata(session)

        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.driver is None:
            return

        self.driver.close()
        self.driver = None

    def new_session(self) -> neo4j.Session:
        return self.driver.session(database=self.database)

    def _wait_for_indexes(self, session: neo4j.Session):
        """
        We don't want to start inserting data until the indices are created.
        """
        session.run("CALL db.awaitIndexes()").consume()

    def create_constraints(self, session: neo4j.Session):
        """
        This method creates all the constraints required for the database.
        Uniqueness constraints implicitly create an index for the constraint as well.
        """
        # MeSH Headings.
        session.run(
            "CREATE CONSTRAINT unique_mesh_heading_ids IF NOT EXISTS "
            "FOR (h:MeshHeading) REQUIRE h.id IS UNIQUE"
        ).consume()

        # Authors.
        session.run(
            "CREATE CONSTRAINT unique_author_names IF NOT EXISTS "
            "FOR (a:Author) REQUIRE a.name IS UNIQUE"
        ).consume()

        # Journals.
        session.run(
            "CREATE CONSTRAINT unique_journal_ids IF NOT EXISTS "
            "FOR (j:Journal) REQUIRE j.id IS UNIQUE"
        ).consume()

        # Articles.
        session.run(
            "CREATE CONSTRAINT unique_article_pmids IF NOT EXISTS "
            "FOR (a:Article) REQUIRE a.pmid IS UNIQUE"
        ).consume()

        # Affiliations.
        session.run(
            "CREATE CONSTRAINT unique_affiliation_names IF NOT EXISTS "
            "FOR (a:Affiliation) REQUIRE a.name IS UNIQUE"
        ).consume()

        # DBMetadata.
        session.run(
            "CREATE CONSTRAINT unique_dbmetadata_versions IF NOT EXISTS "
            "FOR (m:DBMetadata) REQUIRE m.version IS UNIQUE"
        ).consume()

        # User.
        session.run(
            "CREATE CONSTRAINT unique_user_usernames IF NOT EXISTS "
            "FOR (u:User) REQUIRE u.username IS UNIQUE"
        ).consume()

        self._wait_for_indexes(session)

    def drop_indexes(self, session: neo4j.Session) -> int:
        """
        The indexes are unnecessary during the build of the database,
        and would just slow things down.
        """
        existing_indexes: set[str] = set()
        results = session.run("CALL db.indexes()")
        for record in results:
            existing_indexes.add(record["name"])

        delete_indexes = [
            "mesh_name", "journal_title", "article_date", "article_title",
            "dbmetadata_datafile_file", "dbmetadata_meshfile_file"
        ]

        dropped = 0
        for index_name in delete_indexes:
            if index_name in existing_indexes:
                session.run(f"DROP INDEX {index_name}").consume()
                dropped += 1

        self._wait_for_indexes(session)
        return dropped

    def create_indexes(self, session: neo4j.Session):
        """
        This method creates all the constraints and indexes required for the database.
        Uniqueness constraints implicitly create an index for the constraint as well.
        """
        # MeSH Headings.
        session.run(
            "CREATE INDEX mesh_name IF NOT EXISTS "
            "FOR (h:MeshHeading) ON (h.name)"
        ).consume()

        # Journals.
        session.run(
            "CREATE INDEX journal_title IF NOT EXISTS "
            "FOR (j:Journal) ON (j.title)"
        ).consume()

        # Articles.
        session.run(
            "CREATE INDEX article_date IF NOT EXISTS "
            "FOR (a:Article) ON (a.date)"
        ).consume()
        session.run(
            "CREATE INDEX article_title IF NOT EXISTS "
            "FOR (a:Article) ON (a.title)"
        ).consume()

        # DBMetadataDataFile.
        session.run(
            "CREATE INDEX dbmetadata_datafile_file IF NOT EXISTS "
            "FOR (m:DBMetadataDataFile) ON (m.file)"
        ).consume()

        # DBMetadataMeshFile.
        session.run(
            "CREATE INDEX dbmetadata_meshfile_file IF NOT EXISTS "
            "FOR (m:DBMetadataMeshFile) ON (m.file)"
        ).consume()

        self._wait_for_indexes(session)

    def _fetch_metadata(self, session: neo4j.Session):
        """
        Fetches the values to use for the author and article counters from the database.
        """
        self.author_id_counter = IdCounter(self._fetch_max_id(session, "Author") + 1)

    def _fetch_max_id(self, session: neo4j.Session, label: str) -> int:
        """
        Fetches the maximum ID of any nodes matching the given label, or 0 if no nodes could be found.
        """
        result = session.run(
            """
            MATCH (n:{})
            RETURN max(n.id)
            """.format(label)
        ).single()[0]

        # If there are no nodes, then None will be returned.
        return 0 if result is None else result

    @staticmethod
    def read_article_node(node: neo4j.graph.Node) -> DBArticle:
        """ Reads an article node into an Article model object. """
        return DBArticle(
            node["pmid"],
            node["date"].to_native(),
            node["title"]
        )

    @staticmethod
    def read_author_node(node: neo4j.graph.Node) -> DBAuthor:
        """ Reads an author node into an Author model object. """
        return DBAuthor(
            node["name"],
            node["is_collective"],
            author_id=node["id"]
        )

    @staticmethod
    def read_article_author_node(node: neo4j.graph.Node) -> DBArticleAuthor:
        """ Reads an ArticleAuthor node into a DBArticleAuthor model object. """
        return DBArticleAuthor(
            node["author_position"],
            node["is_first_author"],
            node["is_last_author"]
        )

    @staticmethod
    def read_affiliation_node(node: neo4j.graph.Node) -> DBAffiliation:
        """ Reads an Affiliation node into a DBAffiliation model object. """
        return DBAffiliation(
            node["name"]
        )

    def insert_mesh_heading_batch(self, headings: list[DBMeSHHeading], *, max_batch_size=500):
        """
        Inserts a batch of headings into the database.
        """
        if len(headings) == 0:
            return

        # We batch the articles as otherwise we can hit maximum memory issues with Neo4J...
        required_batches = (len(headings) + max_batch_size - 1) // max_batch_size
        headings_per_batch = (len(headings) + required_batches - 1) // required_batches
        total_headings_inserted = 0
        for batch_no in range(required_batches):
            start_index = batch_no * headings_per_batch
            end_index = len(headings) if batch_no == required_batches - 1 else (batch_no + 1) * headings_per_batch
            batch = headings[start_index:end_index]
            total_headings_inserted += len(batch)
            with self.new_session() as session:
                session.write_transaction(self._insert_mesh_heading_batch, batch)

        # Just to be sure...
        assert total_headings_inserted == len(headings)

    def _insert_mesh_heading_batch(self, tx: neo4j.Transaction, headings: list[DBMeSHHeading]):
        """
        Inserts a batch of headings into the database.
        """
        headings_data = []
        for heading in headings:
            headings_data.append({
                "desc_id": heading.descriptor_id,
                "name": heading.name,
                "tree_numbers": heading.tree_numbers
            })

        tx.run(
            """
            UNWIND $headings AS heading
            MERGE (heading_node:MeshHeading {id: heading.desc_id})
            ON CREATE
                SET
                    heading_node.name = heading.name,
                    heading_node.tree_numbers = heading.tree_numbers
            ON MATCH
                SET
                    heading_node.name = heading.name,
                    heading_node.tree_numbers = heading.tree_numbers
            """,
            headings=headings_data
        ).consume()

    def get_mesh_headings(self) -> list[DBMeSHHeading]:
        """
        Fetches the MeSH headings from the database.
        """
        if self._mesh_headings is not None:
            return self._mesh_headings

        with self.new_session() as session:
            results = session.run(
                """
                MATCH (m:MeshHeading)
                RETURN m.id, m.name, m.tree_numbers
                """
            )
            mesh_headings = []
            for id, name, tree_numbers in results:
                mesh_headings.append(DBMeSHHeading(id, name, tree_numbers))

        self._mesh_headings = mesh_headings
        return mesh_headings

    def fetch_db_metadata(self) -> Optional[DBMetadata]:
        """
        Fetches the most recent DBMetadata.
        """
        with self.new_session() as session:
            results = session.run(
                """
                CALL {
                    MATCH (base:DBMetadata)
                    RETURN base
                    ORDER BY base.version DESC
                    LIMIT 1
                }
                CALL {
                    WITH base
                    MATCH (base) -[:META_MESH_SOURCE]-> (meta:DBMetadataMeshFile)
                    RETURN COLLECT(meta) as meta
                }
                CALL {
                    WITH base
                    MATCH (base) -[:META_DATA_SOURCE]-> (data: DBMetadataDataFile)
                    RETURN COLLECT(data) as data
                }
                RETURN base, meta, data
                """
            ).single()
            result = None if results is None or len(results) < 1 else results[0]

        if result is None:
            return None

        base, meta, data = tuple(results)
        return DBMetadata.from_dicts(base, meta, data)

    def write_db_metadata(self, metadata: DBMetadata):
        with self.new_session() as session:
            session.write_transaction(
                self._write_db_metadata,
                metadata
            )

    def _write_db_metadata(self, tx: neo4j.Transaction, metadata: DBMetadata):
        base_data = metadata.to_dict()
        mesh_data = [f.to_processed_dict() for f in [metadata.mesh_file] if f.processed]
        processed_data = [f.to_processed_dict() for f in metadata.data_files if f.processed]
        tx.run(
            """
            CALL {  // Create the current metadata version node.
                MERGE (base: DBMetadata {version: $base_data.version})
                SET base = $base_data
                RETURN base
            }
            CALL {  // Create the nodes that represent the MeSH files.
                WITH base
                UNWIND $mesh_data AS mesh_file WITH base, mesh_file
                MERGE (mesh: DBMetadataMeshFile {
                    year: mesh_file.year,
                    file: mesh_file.file,
                    md5_hash: mesh_file.md5_hash
                })
                CREATE (mesh) <-[:META_MESH_SOURCE]- (base)
            }
            CALL {  // Create the nodes that represent the PubMed data files.
                WITH base
                UNWIND $processed_data AS processed_data_file WITH base, processed_data_file
                MERGE (data: DBMetadataDataFile {
                    category: processed_data_file.category,
                    year: processed_data_file.year,
                    file: processed_data_file.file,
                    md5_hash: processed_data_file.md5_hash,
                    no_articles: processed_data_file.no_articles
                })
                CREATE (data) <-[:META_DATA_SOURCE]- (base)
            }
            """,
            base_data=base_data, mesh_data=mesh_data, processed_data=processed_data
        ).consume()

    def push_new_db_metadata(self, meta: DBMetadata):
        """
        Pushes the new state of the metadata to the database.
        Updates the version and the modification time of the
        metadata object.
        """
        latest_meta = self.fetch_db_metadata()
        if latest_meta is None:
            version = 1
        else:
            version = latest_meta.version + 1

        meta.update_version(version)
        self.write_db_metadata(meta)

    def delete_entire_database_contents(self):
        """
        DANGEROUS FUNCTION: Deletes the entire contents of the Neo4J database!
        """
        with self.new_session() as session:
            session.run(
                """
                MATCH (n)
                CALL {
                    WITH n
                    DETACH DELETE n
                } IN TRANSACTIONS OF 100000 ROWS;
                """
            ).consume()

    def count_nodes(self) -> int:
        """
        Counts the number of nodes in the database.
        """
        with self.new_session() as session:
            return session.run(
                """
                MATCH (n) RETURN COUNT(n)
                """
            ).single()[0]
